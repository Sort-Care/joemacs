;; Object semanticdb-project-database-file
;; SEMANTICDB Tags save file
(semanticdb-project-database-file "semanticdb-project-database-file"
  :tables
  (list
    (semanticdb-table "semanticdb-table"
      :major-mode python-mode
      :tags 
        '( ("numpy" include nil nil [1 19])
            ("asgn1.classifiers.linear_svm" include nil nil [20 62])
            ("asgn1.classifiers.softmax" include nil nil [63 102])
            ("LinearClassifier" type
               (:superclasses ("object")
                :members 
                  ( ("__init__" function
                       (:suite 
                          ( ("self" variable nil (reparse-symbol indented_block_body) [163 176]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [152 156]))                          
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [139 177])
                    ("train" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [190 194])
                            ("X" variable nil (reparse-symbol function_parameters) [196 197])
                            ("y" variable nil (reparse-symbol function_parameters) [199 200])
                            ("learning_rate" variable nil (reparse-symbol function_parameters) [202 215])
                            ("reg" variable nil (reparse-symbol function_parameters) [222 225])
                            ("num_iters" variable nil (reparse-symbol function_parameters) [232 241])
                            ("batch_size" variable nil (reparse-symbol function_parameters) [259 269])
                            ("verbose" variable nil (reparse-symbol function_parameters) [275 282]))                          
                        :documentation "
    Train this linear classifier using stochastic gradient descent.

    Inputs:
    - X: A numpy array of shape (N, D) containing training data; there are N
      training samples each of dimension D.
    - y: A numpy array of shape (N,) containing training labels; y[i] = c
      means that X[i] has label 0 <= c < C for C classes.X
    - learning_rate: (float) learning rate for optimization.
    - reg: (float) regularization strength.
    - num_iters: (integer) number of steps to take when optimizing
    - batch_size: (integer) number of training examples to use at each step.
    - verbose: (boolean) If true, print progress during optimization.

    Outputs:
    A list containing the value of the loss function at each training iteration.
    ")
                        (reparse-symbol indented_block_body) [180 3619])
                    ("predict" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [3634 3638])
                            ("X" variable nil (reparse-symbol function_parameters) [3640 3641]))                          
                        :documentation "
    Use the trained weights of this linear classifier to predict labels for
    data points.

    Inputs:
    - X: N x D array of training data. Each row is a D-dimensional point.

    Returns:
    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional
      array of length N, and each element is an integer giving the predicted
      class.
    ")
                        (reparse-symbol indented_block_body) [3622 4776])
                    ("loss" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [4788 4792])
                            ("X_batch" variable nil (reparse-symbol function_parameters) [4794 4801])
                            ("y_batch" variable nil (reparse-symbol function_parameters) [4803 4810])
                            ("reg" variable nil (reparse-symbol function_parameters) [4812 4815]))                          
                        :documentation "
    Compute the loss function and its derivative.
    Subclasses will override this.

    Inputs:
    - X_batch: A numpy array of shape (N, D) containing a minibatch of N
      data points; each point has dimension D.
    - y_batch: A numpy array of shape (N,) containing labels for the minibatch.
    - reg: (float) regularization strength.

    Returns: A tuple containing:
    - loss as a single float
    - gradient with respect to self.W; an array of the same shape as W
    ")
                        (reparse-symbol indented_block_body) [4779 5319]))                  
                :type "class")
                nil [104 5319])
            ("LinearSVM" type
               (:documentation " A subclass that uses the Multiclass SVM loss function "
                :superclasses ("LinearClassifier")
                :members 
                  ( ("loss" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [5437 5441])
                            ("X_batch" variable nil (reparse-symbol function_parameters) [5443 5450])
                            ("y_batch" variable nil (reparse-symbol function_parameters) [5452 5459])
                            ("reg" variable nil (reparse-symbol function_parameters) [5461 5464]))                          )
                        (reparse-symbol indented_block_body) [5428 5529]))                  
                :type "class")
                nil [5326 5529])
            ("Softmax" type
               (:documentation " A subclass that uses the Softmax + Cross-entropy loss function "
                :superclasses ("LinearClassifier")
                :members 
                  ( ("loss" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [5649 5653])
                            ("X_batch" variable nil (reparse-symbol function_parameters) [5655 5662])
                            ("y_batch" variable nil (reparse-symbol function_parameters) [5664 5671])
                            ("reg" variable nil (reparse-symbol function_parameters) [5673 5676]))                          )
                        (reparse-symbol indented_block_body) [5640 5745]))                  
                :type "class")
                nil [5531 5745]))          
      :file "linear_classifier.py"
      :pointmax 5746
      :fsize 5745
      :lastmodtime '(23166 9993 315414 803000)
      :unmatched-syntax nil)
    (semanticdb-table "semanticdb-table"
      :major-mode python-mode
      :tags 
        '( ("numpy" include nil nil [1 19])
            ("random" include nil nil [20 46])
            ("softmax_loss_naive" function
               (:documentation "
  Softmax loss function, naive implementation (with loops)

  Inputs have dimension D, there are C classes, and we operate on minibatches
  of N examples.

  Inputs:
  - W: A numpy array of shape (D, C) containing weights.
  - X: A numpy array of shape (N, D) containing a minibatch of data.
  - y: A numpy array of shape (N,) containing training labels; y[i] = c means
    that X[i] has label c, where 0 <= c < C.
  - reg: (float) regularization strength

  Returns a tuple of:
  - loss as single float
  - gradient with respect to weights W; an array of same shape as W
  "
                :arguments 
                  ( ("W" variable nil (reparse-symbol function_parameters) [71 72])
                    ("X" variable nil (reparse-symbol function_parameters) [74 75])
                    ("y" variable nil (reparse-symbol function_parameters) [77 78])
                    ("reg" variable nil (reparse-symbol function_parameters) [80 83]))                  )
                nil [48 2189])
            ("softmax_loss_vectorized" function
               (:documentation "
  Softmax loss function, vectorized version.

  Inputs and outputs are the same as softmax_loss_naive.
  "
                :arguments 
                  ( ("W" variable nil (reparse-symbol function_parameters) [2219 2220])
                    ("X" variable nil (reparse-symbol function_parameters) [2222 2223])
                    ("y" variable nil (reparse-symbol function_parameters) [2225 2226])
                    ("reg" variable nil (reparse-symbol function_parameters) [2228 2231]))                  )
                nil [2191 3729]))          
      :file "softmax.py"
      :pointmax 3730
      :fsize 3729
      :lastmodtime '(23167 20976 15553 50000)
      :unmatched-syntax nil)
    (semanticdb-table "semanticdb-table"
      :major-mode python-mode
      :tags 
        '( ("numpy" include nil nil [1 19])
            ("KNearestNeighbor" type
               (:documentation " a kNN classifier with L2 distance "
                :superclasses ("object")
                :members 
                  ( ("__init__" function
                       (:suite 
                          ( ("pass" code nil (reparse-symbol indented_block_body) [124 128]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [113 117]))                          
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [100 129])
                    ("train" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [142 146])
                            ("X" variable nil (reparse-symbol function_parameters) [148 149])
                            ("y" variable nil (reparse-symbol function_parameters) [151 152]))                          
                        :documentation "
    Train the classifier. For k-nearest neighbors this is just 
    memorizing the training data.

    Inputs:
    - X: A numpy array of shape (num_train, D) containing the training data
      consisting of num_train samples each of dimension D.
    - y: A numpy array of shape (N,) containing the training labels, where
         y[i] is the label for X[i].
    ")
                        (reparse-symbol indented_block_body) [132 571])
                    ("predict" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [590 594])
                            ("X" variable nil (reparse-symbol function_parameters) [596 597])
                            ("k" variable nil (reparse-symbol function_parameters) [599 600])
                            ("num_loops" variable nil (reparse-symbol function_parameters) [604 613]))                          
                        :documentation "
    Predict labels for test data using this classifier.

    Inputs:
    - X: A numpy array of shape (num_test, D) containing test data consisting
         of num_test samples each of dimension D.
    - k: The number of nearest neighbors that vote for the predicted labels.
    - num_loops: Determines which implementation to use to compute distances
      between training points and testing points.

    Returns:
    - y: A numpy array of shape (num_test,) containing predicted labels for the
      test data, where y[i] is the predicted label for the test point X[i].  
    ")
                        (reparse-symbol indented_block_body) [578 1551])
                    ("compute_distances_two_loops" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1586 1590])
                            ("X" variable nil (reparse-symbol function_parameters) [1592 1593]))                          
                        :documentation "
    Compute the distance between each test point in X and each training point
    in self.X_train using a nested loop over both the training data and the 
    test data.

    Inputs:
    - X: A numpy array of shape (num_test, D) containing test data.

    Returns:
    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]
      is the Euclidean distance between the ith test point and the jth training
      point.
    ")
                        (reparse-symbol indented_block_body) [1554 3070])
                    ("compute_distances_one_loop" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [3104 3108])
                            ("X" variable nil (reparse-symbol function_parameters) [3110 3111]))                          
                        :documentation "
    Compute the distance between each test point in X and each training point
    in self.X_train using a single loop over the test data.

    Input / Output: Same as compute_distances_two_loops
    ")
                        (reparse-symbol indented_block_body) [3073 4184])
                    ("compute_distances_no_loops" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [4218 4222])
                            ("X" variable nil (reparse-symbol function_parameters) [4224 4225]))                          
                        :documentation "
    Compute the distance between each test point in X and each training point
    in self.X_train using no explicit loops.

    Input / Output: Same as compute_distances_two_loops
    ")
                        (reparse-symbol indented_block_body) [4187 5995])
                    ("predict_labels" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [6017 6021])
                            ("dists" variable nil (reparse-symbol function_parameters) [6023 6028])
                            ("k" variable nil (reparse-symbol function_parameters) [6030 6031]))                          
                        :documentation "
    Given a matrix of distances between test points and training points,
    predict a label for each test point.

    Inputs:
    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]
      gives the distance betwen the ith test point and the jth training point.

    Returns:
    - y: A numpy array of shape (num_test,) containing predicted labels for the
      test data, where y[i] is the predicted label for the test point X[i].  
    ")
                        (reparse-symbol indented_block_body) [5998 8430])
                    ("find_most_common" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [8455 8459])
                            ("closest_y" variable nil (reparse-symbol function_parameters) [8461 8470]))                          )
                        (reparse-symbol indented_block_body) [8434 9136]))                  
                :type "class")
                nil [21 9136]))          
      :file "k_nearest_neighbor.py"
      :pointmax 9138
      :fsize 9137
      :lastmodtime '(23157 56375 957863 311000)
      :unmatched-syntax nil)
    (semanticdb-table "semanticdb-table"
      :major-mode python-mode
      :tags 
        '( ("numpy" include nil nil [1 19])
            ("matplotlib.pyplot" include nil nil [20 51])
            ("TwoLayerNet" type
               (:documentation "
  A two-layer fully-connected neural network. The net has an input dimension of
  N, a hidden layer dimension of H, and performs classification over C classes.
  We train the network with a softmax loss function and L2 regularization on the
  weight matrices. The network uses a ReLU nonlinearity after the first fully
  connected layer.

  In other words, the network has the following architecture:

  input - fully connected layer - ReLU - fully connected layer - softmax

  The outputs of the second fully-connected layer are the scores for each class.
  "
                :superclasses ("object")
                :members 
                  ( ("__init__" function
                       (:suite 
                          ( ("\"\"\"
    Initialize the model. Weights are initialized to small random values and
    biases are initialized to zero. Weights and biases are stored in the
    variable self.params, which is a dictionary with the following keys:

    W1: First layer weights; has shape (D, H)
    b1: First layer biases; has shape (H,)
    W2: Second layer weights; has shape (H, C)
    b2: Second layer biases; has shape (C,)

    Inputs:
    - input_size: The dimension D of the input data.
    - hidden_size: The number of neurons H in the hidden layer.
    - output_size: The number of classes C.
    \"\"\"" code nil (reparse-symbol indented_block_body) [725 1314])
                            ("self" variable nil (reparse-symbol indented_block_body) [1319 1335])
                            ("self" code nil (reparse-symbol indented_block_body) [1340 1406])
                            ("self" variable nil (reparse-symbol indented_block_body) [1411 1452])
                            ("self" code nil (reparse-symbol indented_block_body) [1457 1524])
                            ("self" variable nil (reparse-symbol indented_block_body) [1529 1570]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [666 670])
                            ("input_size" variable nil (reparse-symbol function_parameters) [672 682])
                            ("hidden_size" variable nil (reparse-symbol function_parameters) [684 695])
                            ("output_size" variable nil (reparse-symbol function_parameters) [697 708])
                            ("std" variable nil (reparse-symbol function_parameters) [710 713]))                          
                        :documentation "
    Initialize the model. Weights are initialized to small random values and
    biases are initialized to zero. Weights and biases are stored in the
    variable self.params, which is a dictionary with the following keys:

    W1: First layer weights; has shape (D, H)
    b1: First layer biases; has shape (H,)
    W2: Second layer weights; has shape (H, C)
    b2: Second layer biases; has shape (C,)

    Inputs:
    - input_size: The dimension D of the input data.
    - hidden_size: The number of neurons H in the hidden layer.
    - output_size: The number of classes C.
    "
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [653 1571])
                    ("loss" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1583 1587])
                            ("X" variable nil (reparse-symbol function_parameters) [1589 1590])
                            ("y" variable nil (reparse-symbol function_parameters) [1592 1593])
                            ("reg" variable nil (reparse-symbol function_parameters) [1600 1603]))                          
                        :documentation "
    Compute the loss and gradients for a two layer fully connected neural
    network.

    Inputs:
    - X: Input data of shape (N, D). Each X[i] is a training sample.
    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is
      an integer in the range 0 <= y[i] < C. This parameter is optional; if it
      is not passed then we only return scores, and if it is passed then we
      instead return the loss and gradients.
    - reg: Regularization strength.

    Returns:
    If y is None, return a matrix scores of shape (N, C) where scores[i, c] is
    the score for class c on input X[i].

    If y is not None, instead return a tuple of:
    - loss: Loss (data loss and regularization loss) for this batch of training
      samples.
    - grads: Dictionary mapping parameter names to gradients of those parameters
      with respect to the loss function; has the same keys as self.params.
    ")
                        (reparse-symbol indented_block_body) [1574 6384])
                    ("train" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [6397 6401])
                            ("X" variable nil (reparse-symbol function_parameters) [6403 6404])
                            ("y" variable nil (reparse-symbol function_parameters) [6406 6407])
                            ("X_val" variable nil (reparse-symbol function_parameters) [6409 6414])
                            ("y_val" variable nil (reparse-symbol function_parameters) [6416 6421])
                            ("learning_rate" variable nil (reparse-symbol function_parameters) [6435 6448])
                            ("learning_rate_decay" variable nil (reparse-symbol function_parameters) [6455 6474])
                            ("reg" variable nil (reparse-symbol function_parameters) [6493 6496])
                            ("num_iters" variable nil (reparse-symbol function_parameters) [6503 6512])
                            ("batch_size" variable nil (reparse-symbol function_parameters) [6530 6540])
                            ("verbose" variable nil (reparse-symbol function_parameters) [6546 6553]))                          
                        :documentation "
    Train this neural network using stochastic gradient descent.

    Inputs:
    - X: A numpy array of shape (N, D) giving training data.
    - y: A numpy array f shape (N,) giving training labels; y[i] = c means that
      X[i] has label c, where 0 <= c < C.
    - X_val: A numpy array of shape (N_val, D) giving validation data.
    - y_val: A numpy array of shape (N_val,) giving validation labels.
    - learning_rate: Scalar giving learning rate for optimization.
    - learning_rate_decay: Scalar giving factor used to decay the learning rate
      after each epoch.
    - reg: Scalar giving regularization strength.
    - num_iters: Number of steps to take when optimizing.
    - batch_size: Number of training examples to use per step.
    - verbose: boolean; if true print progress during optimization.
    ")
                        (reparse-symbol indented_block_body) [6387 10142])
                    ("predict" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [10157 10161])
                            ("X" variable nil (reparse-symbol function_parameters) [10163 10164]))                          
                        :documentation "
    Use the trained weights of this two-layer network to predict labels for
    data points. For each data point we predict scores for each of the C
    classes, and assign each data point to the class with the highest score.

    Inputs:
    - X: A numpy array of shape (N, D) giving N D-dimensional data points to
      classify.

    Returns:
    - y_pred: A numpy array of shape (N,) giving predicted labels for each of
      the elements of X. For all i, y_pred[i] = c means that X[i] is predicted
      to have class c, where 0 <= c < C.
    ")
                        (reparse-symbol indented_block_body) [10145 11312]))                  
                :type "class")
                nil [54 11312]))          
      :file "neural_net.py"
      :pointmax 11314
      :fsize 11313
      :lastmodtime '(23169 58237 416525 636000)
      :unmatched-syntax nil)
    (semanticdb-table "semanticdb-table"
      :major-mode python-mode
      :tags 
        '( ("numpy" include nil nil [1 19])
            ("random" include nil nil [20 46])
            ("svm_loss_naive" function
               (:documentation "
  Structured SVM loss function, naive implementation (with loops).

  Inputs have dimension D, there are C classes, and we operate on minibatches
  of N examples.

  Inputs:
  - W: A numpy array of shape (D, C) containing weights.
  - X: A numpy array of shape (N, D) containing a minibatch of data.
  - y: A numpy array of shape (N,) containing training labels; y[i] = c means
    that X[i] has label c, where 0 <= c < C.
  - reg: (float) regularization strength

  Returns a tuple of:
  - loss as single float
  - gradient with respect to weights W; an array of same shape as W
  "
                :arguments 
                  ( ("W" variable nil (reparse-symbol function_parameters) [67 68])
                    ("X" variable nil (reparse-symbol function_parameters) [70 71])
                    ("y" variable nil (reparse-symbol function_parameters) [73 74])
                    ("reg" variable nil (reparse-symbol function_parameters) [76 79]))                  )
                nil [48 2462])
            ("svm_loss_vectorized" function
               (:documentation "
  Structured SVM loss function, vectorized implementation.

  Inputs and outputs are the same as svm_loss_naive.
  "
                :arguments 
                  ( ("W" variable nil (reparse-symbol function_parameters) [2488 2489])
                    ("X" variable nil (reparse-symbol function_parameters) [2491 2492])
                    ("y" variable nil (reparse-symbol function_parameters) [2494 2495])
                    ("reg" variable nil (reparse-symbol function_parameters) [2497 2500]))                  )
                nil [2464 5152]))          
      :file "linear_svm.py"
      :pointmax 5152
      :fsize 5151
      :lastmodtime '(23166 33446 391874 138000)
      :unmatched-syntax nil))
  :file "!Users!jihaoyu!Documents!UMASSCourses!682_Neural_Networks!assignment1!asgn1!classifiers!semantic.cache"
  :semantic-tag-version "2.0"
  :semanticdb-version "2.2")
